{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e1559ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import args_cnn as args\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import config\n",
    "from loss import CCCLoss\n",
    "from utils import Logger, seed_worker\n",
    "from train import train_model\n",
    "from eval import  calc_ccc\n",
    "from model import Model\n",
    "from dataset import MuSeDataset\n",
    "from data_parser import get_data_partition, segment_sample, normalize_data\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13048eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.log_file_name = '{}_[{}]_[{}]_[{}_{}_{}]_[{}_{}]'.format(\n",
    "    datetime.now(tz=tz.gettz()).strftime(\"%Y-%m-%d-%H-%M\"), '_'.join(args.feature_set), args.emo_dim,\n",
    "    args.d_rnn, args.rnn_n_layers, args.rnn_bi, args.lr, args.batch_size)\n",
    "\n",
    "# adjust your paths in config.py\n",
    "args.paths = {'log': os.path.join(config.LOG_FOLDER, args.task),\n",
    "              'data': os.path.join(config.DATA_FOLDER, args.task),\n",
    "              'model': os.path.join(config.MODEL_FOLDER, args.task, args.log_file_name)}\n",
    "if args.predict:\n",
    "    args.paths['predict'] = os.path.join(config.PREDICTION_FOLDER, args.task, args.log_file_name)\n",
    "if args.save:\n",
    "    args.paths['save'] = os.path.join(args.save_path, args.task, args.log_file_name)\n",
    "for folder in args.paths.values():\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "args.paths.update({'features': config.PATH_TO_ALIGNED_FEATURES[args.task],\n",
    "                   'labels': config.PATH_TO_LABELS[args.task],\n",
    "                   'partition': config.PARTITION_FILES[args.task]})\n",
    "\n",
    "# sys.stdout = Logger(os.path.join(args.paths['log'], args.log_file_name + '.txt'))\n",
    "# print(' '.join(sys.argv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "714299fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### load data\n",
    "\n",
    "task = args.task\n",
    "paths = args.paths\n",
    "feature_set = args.feature_set\n",
    "emo_dim = args.emo_dim\n",
    "normalize = args.normalize\n",
    "norm_opts = args.norm_opts\n",
    "win_len = args.win_len\n",
    "hop_len = args.hop_len\n",
    "save = args.save\n",
    "apply_segmentation = args.apply_segmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96ecc550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing data from scratch ...\n"
     ]
    }
   ],
   "source": [
    "feature_path = paths['features']\n",
    "label_path = paths['labels']\n",
    "\n",
    "data_file_name = f'data_{task}_{\"_\".join(feature_set)}_{emo_dim}_{\"norm_\" if normalize else \"\"}{win_len}_' \\\n",
    "    f'{hop_len}{\"_seg\" if apply_segmentation else \"\"}.pkl'\n",
    "data_file = os.path.join(paths['data'], data_file_name)\n",
    "\n",
    "# if os.path.exists(data_file):  # check if file of preprocessed data exists\n",
    "#     print(f'Find cached data \"{os.path.basename(data_file)}\".')\n",
    "#     data = pickle.load(open(data_file, 'rb'))\n",
    "#     return data\n",
    "\n",
    "print('Constructing data from scratch ...')\n",
    "data = {'train': {'feature': [], 'label': [], 'meta': []},\n",
    "        'devel': {'feature': [], 'label': [], 'meta': []},\n",
    "        'test': {'feature': [], 'label': [], 'meta': []}}\n",
    "vid2partition, partition2vid = get_data_partition(paths['partition'])\n",
    "feature_dims = [0] * len(feature_set)\n",
    "\n",
    "feature_idx = 2  # first two columns are timestamp and segment_id, features start with the third column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2192fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for partition, vids in partition2vid.items():\n",
    "    for vid in vids:\n",
    "        sample_data = []\n",
    "        segment_ids_per_step = []  # necessary for MuSe-Sent\n",
    "\n",
    "        # parse features\n",
    "        for i, feature in enumerate(feature_set):\n",
    "            #print(feature)\n",
    "            feature_file = os.path.join(feature_path, feature, vid + '.csv')\n",
    "            assert os.path.exists(\n",
    "                feature_file), f'Error: no available \"{feature}\" feature file for video \"{vid}\": \"{feature_file}\".'\n",
    "            df = pd.read_csv(feature_file)\n",
    "            feature_dims[i] = df.shape[1] - feature_idx\n",
    "            if i == 0:\n",
    "                feature_data = df  # keep timestamp and segment id in 1st feature val\n",
    "                segment_ids_per_step = df.iloc[:, 1]\n",
    "            else:\n",
    "                feature_data = df.iloc[:, feature_idx:]\n",
    "            sample_data.append(feature_data)\n",
    "        data[partition]['feature_dims'] = feature_dims\n",
    "\n",
    "        # parse labels\n",
    "        label_file = os.path.join(label_path, emo_dim, vid + '.csv')\n",
    "        assert os.path.exists(\n",
    "            label_file), f'Error: no available \"{emo_dim}\" label file for video \"{vid}\": \"{label_file}\".'\n",
    "        df = pd.read_csv(label_file)\n",
    "\n",
    "        if task == 'sent':\n",
    "            label = df['class_id'].values\n",
    "            label_stretched = [label[s_id - 1] if not pd.isna(s_id) else pd.NA for s_id in segment_ids_per_step]\n",
    "            label_data = pd.DataFrame(data=label_stretched, columns=[emo_dim])\n",
    "        else:\n",
    "            label_data = pd.DataFrame(data=df['value'].values, columns=[emo_dim])\n",
    "        sample_data.append(label_data)\n",
    "\n",
    "        # concat\n",
    "        sample_data = pd.concat(sample_data, axis=1)\n",
    "        if partition != 'test':\n",
    "            sample_data = sample_data.dropna()\n",
    "\n",
    "        # segment\n",
    "        if apply_segmentation:\n",
    "            if task == 'sent':\n",
    "                seg_type = 'by_segs_only' if partition != 'train' else 'by_segs'\n",
    "                samples = segment_sample(sample_data, win_len, hop_len, seg_type)\n",
    "            elif task in ['wilder', 'physio', 'stress']:\n",
    "                if partition == 'train':\n",
    "                    samples = segment_sample(sample_data, win_len, hop_len, 'normal')\n",
    "                else:\n",
    "                    samples = [sample_data]\n",
    "        else:\n",
    "            if task == 'sent':\n",
    "                samples = segment_sample(sample_data, win_len, hop_len, 'by_segs_only')\n",
    "            else:\n",
    "                samples = [sample_data]\n",
    "\n",
    "        # store\n",
    "        for i, segment in enumerate(samples):  # each segment has columns: timestamp, segment_id, features, labels\n",
    "            n_emo_dims = 1\n",
    "            if len(segment.iloc[:, feature_idx:-n_emo_dims].values) > 0:  # check if there are features\n",
    "                meta = np.column_stack((np.array([int(vid)] * len(segment)),\n",
    "                                        segment.iloc[:, :feature_idx].values))  # video_id, timestamp, segment_id\n",
    "                data[partition]['meta'].append(meta)\n",
    "                data[partition]['label'].append(segment.iloc[:, -n_emo_dims:].values)\n",
    "                data[partition]['feature'].append(segment.iloc[:, feature_idx:-n_emo_dims].values)\n",
    "\n",
    "if normalize:\n",
    "    idx_list = []\n",
    "\n",
    "    assert norm_opts is not None and len(norm_opts) == len(feature_set)\n",
    "    norm_opts = [True if norm_opt == 'y' else False for norm_opt in norm_opts]\n",
    "\n",
    "    print(f'Feature dims: {feature_dims} ({feature_set})')\n",
    "    feature_dims = np.cumsum(feature_dims).tolist()\n",
    "    feature_dims = [0] + feature_dims\n",
    "\n",
    "    norm_feature_set = []  # normalize data per feature and only if norm_opts is True\n",
    "    for i, (s_idx, e_idx) in enumerate(zip(feature_dims[0:-1], feature_dims[1:])):\n",
    "        norm_opt, feature = norm_opts[i], feature_set[i]\n",
    "        if norm_opt:\n",
    "            norm_feature_set.append(feature)\n",
    "            idx_list.append([s_idx, e_idx])\n",
    "\n",
    "    print(f'Normalized features: {norm_feature_set}')\n",
    "    data = normalize_data(data, idx_list)\n",
    "\n",
    "if save:  # save loaded and preprocessed data\n",
    "    print('Saving data...')\n",
    "    pickle.dump(data, open(data_file, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63a6557c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = {}\n",
    "for partition in data.keys():  # one DataLoader for each partition\n",
    "    set = MuSeDataset(data, partition)\n",
    "    batch_size = args.batch_size if partition == 'train' else 1\n",
    "    shuffle = True if partition == 'train' else False  # shuffle only for train partition\n",
    "    data_loader[partition] = torch.utils.data.DataLoader(set, batch_size=batch_size, shuffle=shuffle, num_workers=4,\n",
    "                                                         worker_init_fn=seed_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "518b37d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.d_in = data_loader['train'].dataset.get_feature_dim()\n",
    "args.n_targets = 1\n",
    "criterion = CCCLoss()\n",
    "score_str = 'CCC'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f9a4525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d31d7aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, d_in, d_out, n_layers=1, bi=True, dropout=0):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size=d_in, hidden_size=d_out, bidirectional=bi, num_layers=n_layers, dropout=dropout)\n",
    "\n",
    "    def forward(self, x, x_len):\n",
    "        \n",
    "        x = x.permute(1,0,2)\n",
    "        \n",
    "        x_packed = pack_padded_sequence(x, x_len.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        x_out, (hidden, cell) = self.rnn(x_packed)\n",
    "        x_padded = pad_packed_sequence(x_out, total_length=x.size(1), batch_first=True)[0]\n",
    "        x_padded = x_padded.permute(1,0,2)\n",
    "        return x_padded, hidden, cell\n",
    "\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.rnn = RNN(input_size, hidden_size, num_layers, bi=True, dropout=dropout)\n",
    "        \n",
    "        self.fc_hidden = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.fc_cell = nn.Linear(hidden_size*2, hidden_size)\n",
    "        \n",
    "    def forward(self, x, x_len):\n",
    "        \n",
    "        N, seq_length, num_feats = x.shape\n",
    "        x = x.reshape((seq_length, N, num_feats))\n",
    "        outputs, hidden, cell = self.rnn(x, x_len)\n",
    "        hidden = self.fc_hidden(torch.cat((hidden[0:1], hidden[1:2]), dim=2))\n",
    "        cell = self.fc_cell(torch.cat((cell[0:1], cell[1:2]), dim=2))\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "class OutLayer(nn.Module):\n",
    "    def __init__(self, d_in, d_hidden, d_out, dropout=.0, bias=.0):\n",
    "        super(OutLayer, self).__init__()\n",
    "        self.fc_1 = nn.Sequential(nn.Linear(d_in, d_out), nn.ReLU(True), nn.Dropout(dropout))\n",
    "        #self.fc_2 = nn.Linear(d_hidden, d_out)\n",
    "        #nn.init.constant_(self.fc_2.bias.data, bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.fc_1(x)\n",
    "        return y\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.rnn = nn.LSTM(hidden_size*2 + input_size, hidden_size, num_layers, dropout=dropout)\n",
    "        self.energy = nn.Linear(hidden_size*3,1)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x, x_len, encoder_states, hidden, cell):\n",
    "        N, seq_length, num_feats = x.shape\n",
    "        x = x.reshape((seq_length, N, num_feats))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        sequence_length = encoder_states.shape[0]\n",
    "        h_reshaped = hidden.repeat(sequence_length, 1, 1)\n",
    "        energy = self.relu(self.energy(torch.cat((h_reshaped, encoder_states), dim = 2)))\n",
    "        attention = self.softmax(energy)\n",
    "        \n",
    "        #(seq_length, N, 1)\n",
    "        attention = attention.permute(1,2,0)\n",
    "        #(N, 1, seq_length)\n",
    "        encoder_states = encoder_states.permute(1,0,2)\n",
    "        \n",
    "        \n",
    "        context_vector = torch.bmm(attention, encoder_states).permute(1,0,2)\n",
    "        context_vector = context_vector.repeat(sequence_length, 1, 1)\n",
    "        rnn_input = torch.cat((context_vector, x), dim=2)\n",
    "        #print(rnn_input.shape)\n",
    "        outputs, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
    "        outputs = outputs.permute(1,0,2)\n",
    "        #print(outputs.shape)\n",
    "\n",
    "        return outputs, hidden, cell\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11b302d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, encoder, decoder, out):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.out = out\n",
    "\n",
    "#         self.input_size = input_size\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.output_size = output_size\n",
    "#         self.num_layers = num_layers\n",
    "#         self.fc_hidden = fc_hidden\n",
    "#         self.dropout = dropout\n",
    "#         self.encoder = Encoder(input_size, hidden_size, num_layers, dropout)\n",
    "#         self.decoder = Decoder(input_size, hidden_size, output_size, num_layers, dropout)\n",
    "#         self.out = OutLayer(d_in = hidden_size, d_hidden = fc_hidden, d_out = output_size)\n",
    "        \n",
    "    def forward(self, x, x_len):\n",
    "        encoder_states, hidden, cell = self.encoder(x, x_len)\n",
    "        outputs, hidden, cell = self.decoder(x, x_len, encoder_states, hidden, cell)\n",
    "        outputs = self.out(outputs)\n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5ba0cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder(input_size = 88, hidden_size=32, num_layers=1, dropout = 0)\n",
    "dec = Decoder(input_size=88, hidden_size=32, output_size=1, num_layers=1, dropout=0)\n",
    "outl = OutLayer(d_in = 32, d_hidden = 8, d_out = 1)\n",
    "model = Model(enc, dec, outl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a91d797",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = args.lr\n",
    "regularization = args.regularization\n",
    "train_loader, val_loader, test_loader = data_loader['train'], data_loader['devel'], data_loader['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53cee22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = data_loader['train'], data_loader['devel'], data_loader['test']\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=regularization)\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', patience=5, factor=0.5,\n",
    "                                                    min_lr=1e-5, verbose=True)\n",
    "metric = 'Macro-F1' if task == 'sent' else 'CCC'\n",
    "best_val_loss = float('inf')\n",
    "best_val_score = -1\n",
    "best_model_file = ''\n",
    "early_stop = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a14e9efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(task, model, train_loader, epoch, optimizer, criterion, use_gpu=False):\n",
    "    start_time = time.time()\n",
    "    report_loss, report_size = 0, 0\n",
    "    total_loss, total_size = 0, 0\n",
    "\n",
    "    model.train()\n",
    "    for batch, batch_data in enumerate(train_loader, 1):\n",
    "        features, feature_lens, labels, metas = batch_data\n",
    "        batch_size = features.size(0)\n",
    "\n",
    "        if use_gpu:\n",
    "            model.cuda()\n",
    "            features = features.cuda()\n",
    "            feature_lens = feature_lens.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(features, feature_lens)\n",
    "\n",
    "        loss = criterion(preds[:, :, 0], labels[:, :, 0], feature_lens)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        report_loss += loss.item() * batch_size\n",
    "        report_size += batch_size\n",
    "\n",
    "        avg_loss = report_loss / report_size\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(\n",
    "            f\"Epoch:{epoch:>3} | Batch: {batch:>3} | Lr: {optimizer.state_dict()['param_groups'][0]['lr']:>1.5f}\"\n",
    "            f\" | Time used(s): {elapsed_time:>.1f} | Training loss: {avg_loss:>.4f}\")\n",
    "\n",
    "        total_loss += report_loss\n",
    "        total_size += report_size\n",
    "        report_loss, report_size, start_time = 0, 0, time.time()\n",
    "\n",
    "    train_loss = total_loss / total_size\n",
    "    return train_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fe2c00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(task, model, data_loader, criterion, use_gpu=False, predict=False, prediction_path=''):\n",
    "    losses, sizes = 0, 0\n",
    "    full_preds = []\n",
    "    if predict:\n",
    "        full_metas = []\n",
    "    else:\n",
    "        full_labels = []\n",
    "\n",
    "    if task == 'sent':\n",
    "        full_logits = []\n",
    "        full_metas_stepwise = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch, batch_data in enumerate(data_loader, 1):\n",
    "            features, feature_lens, labels, metas = batch_data\n",
    "            batch_size = features.size(0)\n",
    "\n",
    "            if use_gpu:\n",
    "                model.cuda()\n",
    "                features = features.cuda()\n",
    "                feature_lens = feature_lens.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            preds = model(features, feature_lens)\n",
    "\n",
    "            if predict:\n",
    "                full_metas.append(metas.detach().squeeze(0).numpy())\n",
    "                if task == 'sent':\n",
    "                    full_metas_stepwise.append(metas_stepwise.detach().squeeze(0).numpy())\n",
    "                    full_logits.append(logits_stepwise.cpu().detach().squeeze(0).numpy())\n",
    "            else:\n",
    "                loss = criterion(preds[:, :, 0], labels[:, :, 0], feature_lens)\n",
    "\n",
    "                losses += loss.item() * batch_size\n",
    "                sizes += batch_size\n",
    "\n",
    "                full_labels.append(labels.cpu().detach().squeeze(0).numpy())\n",
    "            full_preds.append(preds.cpu().detach().squeeze(0).numpy())\n",
    "\n",
    "        if predict:\n",
    "            write_predictions(full_metas, full_preds, prediction_path)\n",
    "            return\n",
    "        else:\n",
    "            score = calc_ccc(full_preds, full_labels)\n",
    "            total_loss = losses / sizes\n",
    "            return total_loss, score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21215b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/100 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 6520, 12636, 28912, 32748) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    989\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 990\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    991\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\queues.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    113\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\JEJOMA~1\\AppData\\Local\\Temp/ipykernel_18144/3125592413.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JEJOMA~1\\AppData\\Local\\Temp/ipykernel_18144/567462694.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(task, model, train_loader, epoch, optimizer, criterion, use_gpu)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_lens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1186\u001b[1;33m             \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1187\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1150\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1152\u001b[1;33m                 \u001b[0msuccess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1001\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfailed_workers\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1002\u001b[0m                 \u001b[0mpids_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m', '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfailed_workers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1003\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'DataLoader worker (pid(s) {}) exited unexpectedly'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpids_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1004\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1005\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 6520, 12636, 28912, 32748) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "epochs = args.epochs\n",
    "use_gpu = args.use_gpu\n",
    "import time\n",
    "current_seed = args.seed\n",
    "seed = args.seed\n",
    "\n",
    "for epoch in tqdm(range(1, epochs + 1)):\n",
    "    train_loss = train(task, model, train_loader, epoch, optimizer, criterion, use_gpu)\n",
    "    val_loss, val_score = evaluate(task, model, val_loader, criterion, use_gpu)\n",
    "\n",
    "    print('-' * 50)\n",
    "    print(f'Epoch:{epoch:>3} | [Train] | Loss: {train_loss:>.4f}')\n",
    "    print(f'Epoch:{epoch:>3} |   [Val] | Loss: {val_loss:>.4f} | [{metric}]: {val_score:>7.4f}')\n",
    "    print('-' * 50)\n",
    "\n",
    "    if val_score > best_val_score:\n",
    "        early_stop = 0\n",
    "        best_val_score = val_score\n",
    "        best_val_loss = val_loss\n",
    "        #best_model_file = save_model(model, model_path, current_seed)\n",
    "\n",
    "    else:\n",
    "        early_stop += 1\n",
    "        if early_stop >= 15:\n",
    "            print(f'Note: target can not be optimized for 15 consecutive epochs, early stop the training process!')\n",
    "            print('-' * 50)\n",
    "            break\n",
    "\n",
    "    lr_scheduler.step(1 - np.mean(val_score))\n",
    "\n",
    "print(f'Seed {current_seed} | '\n",
    "      f'Best [Val {metric}]:{best_val_score:>7.4f} | Loss: {best_val_loss:>.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e690b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
